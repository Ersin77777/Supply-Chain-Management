# from __future__ import annotations
# from dataclasses import dataclass
# from pathlib import Path
# import numpy as np
# import pandas as pd
# import joblib

# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score
# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix


# # ---------- Data filters ----------
# def filter_train_data_for_delivery(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     For delivery_status model:
#     - drop invalid, extreme_date, open orders
#     - require delivery_status non-null
#     """
#     return df[(df["is_invalid"] == 0) & (df["is_extreme_date"] == 0) & (df["is_open_order"] == 0)].dropna(
#         subset=["delivery_status"]
#     )


# def filter_train_data_for_quantity(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     For quantity_status model:
#     - drop invalid, extreme_date, open orders
#     - require quantity_status non-null
#     """
#     return df[(df["is_invalid"] == 0) & (df["is_extreme_date"] == 0) & (df["is_open_order"] == 0)].dropna(
#         subset=["quantity_status"]
#     )


# # ---------- Evaluation ----------
# def evaluate_model(model, X, y, stratified: bool = True) -> float:
#     cv = (
#         StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
#         if stratified
#         else KFold(n_splits=5, shuffle=True, random_state=42)
#     )

#     scores = cross_val_score(
#         estimator=model,
#         X=X,
#         y=y,                      
#         cv=cv,
#         scoring="f1_macro",
#         n_jobs=-1,
#         error_score="raise"      
#     )
#     return scores.mean()



# def model_report(model, X, y, title: str = "") -> None:
#     X_train, X_test, y_train, y_test = train_test_split(
#         X, y, test_size=0.2, random_state=42, stratify=y
#     )
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)

#     print("\n" + "=" * 70)
#     print(f"{title} - Classification Report")
#     print(classification_report(y_test, y_pred, digits=3))
#     print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
#     print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
#     print("=" * 70)


# @dataclass
# class BestModelResult:
#     name: str
#     model: object
#     cv_macro_f1: float


# def evaluate_all_models(
#     models: dict,
#     X,
#     y,
#     company: str,
#     taskname: str,
#     stratified: bool = True,
# ):
#     print(f"\n====== {company} - {taskname} (5-Fold CV macro-F1) ======")

#     # âœ… å¼ºåˆ¶ï¼šè¿™é‡Œç»Ÿä¸€åš label encoding
#     le = LabelEncoder()
#     y_enc = le.fit_transform(y)

#     best_name, best_model, best_score = None, None, -1.0

#     for name, model in models.items():
#         f1 = evaluate_model(
#             model=model,
#             X=X,
#             y=y_enc,              # ğŸ”‘ åªä¼ æ•°å€¼æ ‡ç­¾
#             stratified=stratified
#         )
#         print(f"{name}: {f1:.3f}")

#         if f1 > best_score:
#             best_name = name
#             best_model = model
#             best_score = f1

#     print(f"\n>> Best Model for {company} {taskname}: {best_name} (F1={best_score:.3f})")

#     return {
#         "best_model_name": best_name,
#         "best_model": best_model,
#         "label_encoder": le,
#         "best_score": best_score,
#     }



# # ---------- Persistence ----------
# def save_model(model, out_path: str | Path) -> None:
#     out_path = Path(out_path)
#     out_path.parent.mkdir(parents=True, exist_ok=True)
#     joblib.dump(model, out_path)


# def load_model(path: str | Path):
#     return joblib.load(path)




from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Union
import collections

import numpy as np
import pandas as pd
import joblib

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.base import BaseEstimator, ClassifierMixin

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline


# ---------- Data filters ----------
def filter_train_data_for_delivery(df: pd.DataFrame) -> pd.DataFrame:
    return df[(df["is_invalid"] == 0) & (df["is_extreme_date"] == 0) & (df["is_open_order"] == 0)].dropna(
        subset=["delivery_status"]
    )

def filter_train_data_for_quantity(df: pd.DataFrame) -> pd.DataFrame:
    return df[(df["is_invalid"] == 0) & (df["is_extreme_date"] == 0) & (df["is_open_order"] == 0)].dropna(
        subset=["quantity_status"]
    )


# ---------- Helper: Robust SMOTE ----------
def get_robust_smote(y_train):
    """
    æ™ºèƒ½è·å– SMOTE å¯¹è±¡ã€‚
    å¦‚æœæœ€å°ç±»åˆ«çš„æ ·æœ¬æ•°éå¸¸å°‘ï¼ˆä¾‹å¦‚å°‘äº6ä¸ªï¼‰ï¼Œ
    SMOTE é»˜è®¤çš„ k_neighbors=5 ä¼šæŠ¥é”™ã€‚
    æ­¤å‡½æ•°è‡ªåŠ¨å°† k_neighbors è°ƒä½ï¼Œç¡®ä¿ä¸ä¼šæŠ¥é”™ã€‚
    """
    # ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°
    counts = collections.Counter(y_train)
    min_samples = min(counts.values())
    
    # åŠ¨æ€è°ƒæ•´ k_neighbors
    # å¦‚æœæœ€å°‘ç±»åˆ«åªæœ‰ 2 ä¸ªæ ·æœ¬ï¼Œkåªèƒ½è®¾ä¸º 1
    # å¦‚æœæœ‰ 6 ä¸ªä»¥ä¸Šï¼Œk è®¾ä¸ºé»˜è®¤ 5
    k = min(min_samples - 1, 5)
    if k < 1: k = 1
    
    return SMOTE(random_state=42, k_neighbors=k)


# ---------- Evaluation ----------
def evaluate_model(model, X, y, stratified: bool = True, use_smote: bool = False) -> float:
    cv = (
        StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        if stratified
        else KFold(n_splits=5, shuffle=True, random_state=42)
    )

    if use_smote:
        # åœ¨ Pipeline ä¸­æ— æ³•åŠ¨æ€è·å– y æ¥è°ƒæ•´ k_neighborsï¼Œ
        # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¿å®ˆçš„ SMOTE é…ç½® (k_neighbors=1) 
        # æˆ–è€…ä½ ä¹Ÿå¯ä»¥ç¼–å†™è‡ªå®šä¹‰é‡‡æ ·å™¨ï¼Œä½† k=1 å¯¹äºæå°‘æ ·æœ¬æ˜¯å®‰å…¨çš„ã€‚
        pipeline = ImbPipeline([
            ('smote', SMOTE(random_state=42, k_neighbors=1)), 
            ('model', model)
        ])
        estimator = pipeline
    else:
        estimator = model

    scores = cross_val_score(
        estimator=estimator,
        X=X,
        y=y,                      
        cv=cv,
        scoring="f1_macro",
        n_jobs=-1,
        error_score="raise"      
    )
    return scores.mean()


def model_report(model, X, y, label_encoder: LabelEncoder = None, title: str = "", use_smote: bool = False) -> None:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    if use_smote:
        # åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥æ ¹æ®åˆ‡åˆ†åçš„ y_train åŠ¨æ€è°ƒæ•´ SMOTE
        sm = get_robust_smote(y_train)
        print(f"Applying SMOTE with k_neighbors={sm.k_neighbors}...")
        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
        model.fit(X_train_res, y_train_res)
    else:
        model.fit(X_train, y_train)
        
    y_pred = model.predict(X_test)

    if label_encoder is not None:
        y_test_labels = label_encoder.inverse_transform(y_test)
        y_pred_labels = label_encoder.inverse_transform(y_pred)
    else:
        y_test_labels = y_test
        y_pred_labels = y_pred

    print("\n" + "=" * 70)
    print(f"{title} - Classification Report")
    print(classification_report(y_test_labels, y_pred_labels, digits=3))
    print(f"Accuracy: {accuracy_score(y_test_labels, y_pred_labels):.3f}")
    print("Confusion Matrix:\n", confusion_matrix(y_test_labels, y_pred_labels))
    print("=" * 70)


def evaluate_all_models(
    models: dict,
    X,
    y,
    company: str,
    taskname: str,
    stratified: bool = True,
    use_smote: bool = False
):
    print(f"\n====== {company} - {taskname} (5-Fold CV macro-F1) [SMOTE={use_smote}] ======")

    le = LabelEncoder()
    y_enc = le.fit_transform(y)

    best_name = None
    best_model = None
    best_score = -1.0

    for name, model in models.items():
        try:
            f1 = evaluate_model(
                model=model,
                X=X,
                y=y_enc,
                stratified=stratified,
                use_smote=use_smote
            )
            print(f"{name}: {f1:.3f}")

            if f1 > best_score:
                best_name = name
                best_model = model
                best_score = f1
        except Exception as e:
            print(f"{name}: Failed - {str(e)}")

    if best_model is not None:
        print(f"\n>> Best Model for {company} {taskname}: {best_name} (F1={best_score:.3f})")
        model_report(
            best_model, 
            X, 
            y_enc, 
            label_encoder=le, 
            title=f"{company} {taskname} ({best_name})",
            use_smote=use_smote
        )
        return {
            "best_model_name": best_name,
            "best_model": best_model,
            "label_encoder": le,
            "best_score": best_score,
        }
    else:
        print("\n>> No models were successfully evaluated.")
        return None

# ---------- Persistence ----------
def save_model(model, out_path: Union[str, Path]) -> None:
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, out_path)

def load_model(path: Union[str, Path]):
    return joblib.load(path)